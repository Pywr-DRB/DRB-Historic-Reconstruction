{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is a demo. \n",
    "\n",
    "Process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import sys\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the QPPQ model\n",
    "from QPPQModel import StreamflowGenerator\n",
    "\n",
    "# Directory to pywrdrb project\n",
    "pywrdrb_directory = '../Pywr-DRB/'\n",
    "sys.path.append(pywrdrb_directory)\n",
    "\n",
    "from pywrdrb.pywr_drb_node_data import obs_pub_site_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model generation specifications\n",
    "full_date_range = ('1945-01-01', '2023-06-01')\n",
    "N_ENSEMBLE = 5\n",
    "K = 5\n",
    "donor_fdc = 'nhmv10'\n",
    "hru_scaled = False\n",
    "\n",
    "# Constants\n",
    "cms_to_mgd = 22.82\n",
    "fdc_quantiles = [0.0003, 0.005, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.93, 0.95, 0.97, 0.995, 0.9997]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data \n",
    "Q = pd.read_csv(f'./data/historic_unmanaged_streamflow_1900_2023_cms.csv', sep = ',', index_col = 0, parse_dates=True)*cms_to_mgd\n",
    "nhm_flow = pd.read_csv(f'{pywrdrb_directory}/input_data/gage_flow_nhmv10.csv', sep =',',  index_col = 0, parse_dates=True)\n",
    "nwm_flow = pd.read_csv(f'{pywrdrb_directory}/input_data/gage_flow_nwmv21.csv', sep =',',  index_col = 0, parse_dates=True)\n",
    "\n",
    "prediction_locations = pd.read_csv(f'./data/prediction_locations.csv', sep = ',', index_col=0)\n",
    "gauge_meta = pd.read_csv(f'./data/drb_unmanaged_usgs_metadata.csv', sep = ',', dtype = {'site_no':str})\n",
    "gauge_meta.set_index('site_no', inplace=True)\n",
    "\n",
    "# Some gauge data is faulty\n",
    "gauge_meta.loc['01414000', 'begin_date'] = '1996-12-05'\n",
    "gauge_meta.loc['0142400103', 'begin_date'] = '1996-12-05'\n",
    "\n",
    "# Get estiamtes of FDCs at all nodes; to be used for QPPQ when no data is available\n",
    "node_fdcs = pd.DataFrame(index = prediction_locations.index, columns=fdc_quantiles)\n",
    "if donor_fdc == 'nhmv10':\n",
    "    fdc_donor_flow = nhm_flow\n",
    "elif donor_fdc == 'nwmv21':\n",
    "    fdc_donor_flow = nwm_flow\n",
    "else:\n",
    "    print('Invalid donor_fdc specification. Options: nhmv10, nwmv21')\n",
    "\n",
    "for i, node in enumerate(prediction_locations.index):\n",
    "    node_fdcs.loc[node, :] = np.quantile(fdc_donor_flow.loc[:,node], fdc_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outflow gauges from flow data\n",
    "for node, sites in obs_pub_site_matches.items():\n",
    "    if f'USGS-{node}' in Q.columns:\n",
    "        print(f'Removing {node} from data.')\n",
    "        Q = Q.drop(f'USGS-{node}', axis=1)\n",
    "        \n",
    "# Make sure other inflow gauges are in the dataset\n",
    "missing = 0\n",
    "for node, sites in obs_pub_site_matches.items():\n",
    "    if sites is not None:\n",
    "        for s in sites:\n",
    "            if f'USGS-{s}' not in Q.columns:\n",
    "                print(f'Site {s} for node {node} is not available')\n",
    "#assert(missing == 0), 'Atleast one of the inflow gauge timeseries if not available in the data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7679.778330551059"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_fdcs.loc['delTrenton'].max()*0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up QPPQ\n",
    "reconstructed_sites = []\n",
    "for node, sites in obs_pub_site_matches.items():\n",
    "    if node == 'delDRCanal':\n",
    "        pass\n",
    "    elif sites is None:\n",
    "        reconstructed_sites.append(node)\n",
    "    else:\n",
    "        for s in sites:\n",
    "            reconstructed_sites.append(s)\n",
    "\n",
    "# Intialize storage\n",
    "max_daterange = pd.date_range('1945-01-01', '2022-12-31')\n",
    "max_annual_NA_fill = 20\n",
    "Q_reconstructed = pd.DataFrame(index=max_daterange, columns = reconstructed_sites)\n",
    "\n",
    "N_YEARS = int(np.floor(len(max_daterange)/365))\n",
    "\n",
    "starts = [f'{1945+i}-01-01' for i in range(N_YEARS)]\n",
    "ends = [f'{1945+i}-12-31' for i in range(N_YEARS)]\n",
    "daterange_subsets = np.vstack([starts, ends]).transpose()\n",
    "assert(pd.to_datetime(daterange_subsets[-1,-1]).date() <= Q.index.max().date()), 'The historic data must be more recent than QPPQ daterange.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QPPQ prediction\n",
    "# Generate 1 year at a time, to maximize the amount of data available for each years QPPQ\n",
    "for i, dates in enumerate(daterange_subsets):\n",
    "    # Run predictions one location at a time\n",
    "    for node, sites in obs_pub_site_matches.items():\n",
    "        \n",
    "        # Pull gauges that have flow during daterange\n",
    "        Q_subset = Q.loc[dates[0]:dates[1], :].dropna(axis=1)\n",
    "        subset_sites = [f'{i.split(\"-\")[1]}' for i in Q_subset.columns]\n",
    "        gauge_meta_subset = gauge_meta.loc[subset_sites, :]\n",
    "        gauge_meta_subset.index = Q_subset.columns\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = StreamflowGenerator(K= K,\n",
    "                                    observed_flow = Q_subset, \n",
    "                                    observation_locations=gauge_meta_subset,\n",
    "                                    probabalistic_sample = False)\n",
    "\n",
    "        # Handle sites with historic data\n",
    "        if sites is not None:\n",
    "            for s in sites:\n",
    "                # First, use observation data if available\n",
    "                number_of_nas = Q.loc[dates[0]:dates[1], f'USGS-{s}'].isna().sum()\n",
    "                if (number_of_nas == 0):\n",
    "                    Q_reconstructed.loc[dates[0]:dates[1], s] = Q.loc[dates[0]:dates[1], f'USGS-{s}'].values\n",
    "                elif (number_of_nas <= max_annual_NA_fill):\n",
    "                    # print(f'Filling {number_of_nas} NAs for site {s} using median.')\n",
    "                    Q_reconstructed.loc[dates[0]:dates[1], s] = Q.loc[dates[0]:dates[1], f'USGS-{s}'].values\n",
    "                    \n",
    "                    # Fill NA using median                    \n",
    "                    na_indices = Q.loc[dates[0]:dates[1],:].loc[Q.loc[dates[0]:dates[1], f'USGS-{s}'].isna(), :].index\n",
    "                    median_flow = np.median(Q.loc[dates[0]:dates[1], :].loc[~Q.loc[dates[0]:dates[1], f'USGS-{s}'].isna(), f'USGS-{s}'])\n",
    "                    Q_reconstructed.loc[na_indices.date, s] = median_flow\n",
    "                        \n",
    "                # If flow data isn't available, use historic observation to generate FDC and make PUB predictions\n",
    "                else:\n",
    "                    # print(f'Using partial record for {s} during {dates}')\n",
    "                    location = gauge_meta.loc[s, ['long', 'lat']].values\n",
    "                    incomplete_site_flows = Q.loc[:, f'USGS-{s}'].dropna(axis=0)\n",
    "                    \n",
    "                    # Only use site flows for FDC if longer than 10-year record\n",
    "                    if len(incomplete_site_flows)/365 >= 10:\n",
    "                        fdc = np.quantile(incomplete_site_flows.values, fdc_quantiles)\n",
    "                    else:\n",
    "                        fdc = node_fdcs.loc[node, :].astype('float').values            \n",
    "        \n",
    "                    Q_reconstructed.loc[dates[0]:dates[1], s] = model.predict_streamflow(location, fdc).values.flatten()\n",
    "        else:\n",
    "            # print(f'Full PUB for {s} during {dates}')\n",
    "            location = prediction_locations.loc[node, ['long', 'lat']].values\n",
    "            fdc = node_fdcs.loc[node, :].astype('float').values\n",
    "\n",
    "            Q_reconstructed.loc[dates[0]:dates[1], node] = model.predict_streamflow(location, fdc).values.flatten()\n",
    "\n",
    "assert(Q_reconstructed.isna().sum().sum() == 0), 'There are NA values in the reconstruction.'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export\n",
    "Q_reconstructed.to_csv(f'./outputs/historic_reconstruction_daily_{donor_fdc}_mgd.csv', sep = ',')\n",
    "Q_reconstructed.to_csv(f'{pywrdrb_directory}/input_data/modeled_gages/historic_reconstruction_daily_{donor_fdc}_mgd.csv', sep = ',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Go to the `reconstruction_diagnostics.ipynb` to check the quality of the reconstruction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
